---
title: "AutoML in Practice"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  blogdown::html_page:
    fig_width: 6
    toc: yes
    toc_depth: 3
slug: Using H2O and LIME
subtitle: Employee Churn Prediction
tags:
- machine learning
- automl
categories: R
type: post
---

# Introduction: The Problem

Dataset Source: https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/


# Required packages

```{r, message=FALSE, warning=FALSE, results='hide'}

library(h2o)          # High performance machine learning
library(lime)         # Explaining black-box models
library(recipes)      # Creating ML preprocessing recipes
library(tidyverse)    # Set of pkgs for data science: dplyr, ggplot2, purrr, tidyr, ...
library(tidyquant)    # Financial time series pkg - Used for theme_tq ggplot2 theme
library(glue)         # Pasting text
library(cowplot)      # Handling multiple ggplots
library(GGally)       # Data understanding - visualizations
library(skimr)        # Data understanding - summary information
library(fs)           # Working with the file system - directory structure
library(readxl)       # Reading excel files
library(forcats)      # Tools for working with categorical variables
library(writexl)      # Writing to excel files

```

**Reading dataset in R**

```{r, message=FALSE, warning=FALSE}
path_train <- "00_Data/telco_train.xlsx"
train_raw_tbl<-read_excel(path_train,sheet = 1)
colnames(train_raw_tbl)
```

# Business Science Problem Framework 

![Credits: Business Science University](https://www.business-science.io/assets/2018-06-19_BSPF/bspf_top.PNG)


## Phase 1 : Business Understanding

### View Business as Machine 

**Step 1: Isolate Business Units**

In realistic situation, we may not have the data on all the columns listed, so subsetting data here
for fields of interest like EmployeeNumber,  Department,  JobRole,  Attrition

```{r, message=FALSE, warning=FALSE}

dept_job_role_tbl <- train_raw_tbl %>% select(EmployeeNumber,Department,JobRole,Attrition)

dept_job_role_tbl
```

In this problem, potential candidates of business units are **Department** and **Job Roles**

**Step 2: Objectives**

Our business objective is to "**Retain High Performers**"  

```{r, message=FALSE, warning=FALSE}
dept_job_role_tbl %>% 
  group_by(Attrition) %>% 
  summarize(n=n()) %>% 
  ungroup() %>% 
  mutate(pct=n/sum(n))
```
 
 
**Step 3: Collect outcomes in terms of feedback. Feedback identifies problems**

This implies 16% Attrition. We should understand that not all attrition is bad, employee productivity
varies. Some attrition is necessary, lets say poor job employee fit and some attrition is bad, say 
loosing a high performer. We need  to assess whether this 16% is good attrition or bad attrition

### Understand the Drivers 

**Step 1: Investigate if objectives are being met**

Organization has 16% Attrition rate

**Step 2: Synthesize outcomes**  

**High counts and High Percentages**

```{r, message=FALSE, warning=FALSE}
options(dplyr.width = Inf)

dept_job_role_tbl %>% 
  
  group_by(Department,Attrition) %>% 
  summarize(n=n()) %>% 
  ungroup() %>% 
  
  group_by(Department) %>%
  mutate(pct=n/sum(n))

```

We observe that there may be something going on in department since there are different percentages 
of attrition within different departments.

**Attrition by Department & Job Role**  

```{r, message=FALSE, warning=FALSE}

dept_job_role_tbl %>% 
  
  group_by(Department,JobRole, Attrition) %>% 
  summarize(n=n()) %>% 
  ungroup() %>% 
  
  group_by(Department,JobRole) %>%
  mutate(pct=n/sum(n)) %>%
  ungroup() %>%
  
  filter(Attrition %in% "Yes" )
```

We observe high counts and high percentages by Job Role and Departments, for ex: high counts in sales department

**Step 3: Hypothesize drivers**  

In this step, we should work with either subject matter expert or stakeholders to decide what drives the business. In this case, we have **Job Role** and **Department**


### Measure the Drivers 

**Step 1: Collect information on employee attrition : Ongoing**

In this case, we should ask following: 

* Is data available ?  
* Treat information as assets   
* Build Strategic Databases  

**Step 2: Develop KPI's**

* In this step, we can either use **benchmark industry metrics** if available or  
* Develop **internal metrics** using goals
* **Industry KPI: 8.8% attrition rate**    
* We will consider this as a conservative KPI that indicates a major problem if exceeded  

### Uncover Problems and opportunities

**Step 1: Evaluate performance vs KPIs**

**Attrition Cost** : Cost incurred when an employee leaves   

Calculating **Attrition cost** which can be broken down into following:   

* Direct Costs  
* Lost Productivity Costs  
* Savings of salary and Benefits (Cost Reduction)  

Cost per employee = Direct Costs + Lost Productivity Costs + Savings of salary and Benefits  

Total Attrition Cost = No. of employees * Cost per employee  

```{r, message=FALSE, warning=FALSE}

calculate_attrition_cost<- function (
  #Employee  
  n                  = 1,
  salary             = 80000,
  
  
  #Direct Costs
  separation_cost          = 500,
  vacancy_cost             = 10000,
  acquisition_cost         = 4900,
  placement_cost           = 3500, 
  
  
  #Productivity Costs
  net_revenue_per_employee = 250000, 
  workdays_per_year        = 240,
  workdays_position_open   = 40,
  workdays_onboarding      = 60, 
  onboarding_efficiency    = 0.50
  
){
  
  #Direct Costs
  direct_cost <- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)
  
  #Lost Productivity Costs
  productivity_cost <- net_revenue_per_employee /workdays_per_year *
    (workdays_position_open + workdays_onboarding * onboarding_efficiency)
  
  # Savings of salary and Benefits (Cost Reduction)
  salary_benefit_reduction <- salary / workdays_per_year * workdays_position_open
  
  #Estimated Turnover per Employee 
  cost_per_employee <- direct_cost + productivity_cost - salary_benefit_reduction
  
  # Total Cost of Employee Turnover 
  total_cost <- n * cost_per_employee
  
  return(total_cost)
}
```

```{r, message=FALSE, warning=FALSE}

calculate_attrition_cost()
```

**Attriton Cost per person is : 78483.33**  

**Attriton Cost for 200 employees is : 15,696,667 ~ 15.7 million**

This implies that it is a **$15.7 million/year** problem for an organisation that loses 200 employees per year

**Workflow of Attrition**


```{r, message=FALSE, warning=FALSE}

count_to_pct <-  function(data,...,col=n){
  
  grouping_vars_expr <- quos(...) 
  col_expr <- enquo(col)  
  
  ret <- data %>%
    group_by(!!! grouping_vars_expr) %>%
    mutate(pct = (!! col_expr) /sum(!! col_expr)) %>%
    ungroup()
  
  return(ret)
}

assess_attrition <- function(data, attrition_col, attrition_value, baseline_pct) {
      
      attrition_col_expr <- enquo(attrition_col)
        
      data %>%
            filter((!! attrition_col_expr) %in% attrition_value ) %>%
            arrange(desc(pct)) %>%
            mutate (
                above_industry_avg =  case_when(
                pct> baseline_pct ~ "Yes" ,
                TRUE ~ "No"
            )
          )
            
}
```

**Step 2: Highlight potential problem areas**

Calculating Attrition cost by **Department** & **JobRole**

```{r, message=FALSE, warning=FALSE}

dept_job_role_tbl %>% 
  
  count(Department, JobRole, Attrition) %>% 
  
  count_to_pct(Department, JobRole)%>%
  
  assess_attrition(Attrition, attrition_value = "Yes", baseline_pct = 0.088) %>% 
  
  mutate(cost_of_attrition= calculate_attrition_cost(n=n,80000))
```

We can observe that **Sales Representative** (~2 million) and **Laboratory Technician** (~3.8 million) have higher cost of attrition within Sales and Research & Development department even with lower attrition rate.  

10% reduction in turnover saves the organization **$1.6 million / year**. So, this problem is worth solving for an organization.  

**Step 3: Review process and consider what could be missed or needed to answer questions**

**Visualizing of Attrition cost by JobRole**

```{r, message=FALSE, warning=FALSE}

plot_attrition <- function(data,...,.value,
                           fct_reorder = TRUE,
                           fct_rev = FALSE,
                           include_lbl = TRUE,
                           color = palette_light()[[1]],
                           units = c("0", "K", "M")) {
  
  
  #Inputs
  group_vars_expr <- quos(...)
  if(length(group_vars_expr)==0)
    group_vars_expr <- quos(rlang::sym(colnames(data)[[1]]))
  
  value_expr <- enquo(.value)
  value_name <- quo_name(value_expr)
  
  units_val <- switch(units[[1]],
                      "M" = 1e6,
                      "K" = 1e3,
                      "0" = 1) 
  if(units[[1]] == "0") units <- ""
  
  #Data Manipulation
  usd <- scales::dollar_format(prefix = "$" , largest_with_cents = 1e3)
  
  data_manipulated <- data %>%
    mutate(name = str_c(!!! group_vars_expr, sep = ": ") %>% as_factor()) %>%
    mutate(value_text = str_c(usd(!! value_expr / units_val ), units[[1]] , sep = ""))
  
  if(fct_reorder){
    data_manipulated <-  data_manipulated %>% 
      mutate(name = forcats::fct_reorder(name, !! value_expr)) %>% 
      arrange(name)
  }
    
  if(fct_rev){
    data_manipulated <-  data_manipulated %>% 
      mutate(name = forcats::fct_rev(name)) %>% 
      arrange(name)
  }
  
  
  #Visualization
  
  g <- data_manipulated  %>%
    ggplot(aes_string(x = value_name, y = "name")) +
    geom_segment(aes(xend = 0, yend = name), color = color) +
    geom_point(aes_string(size = value_name), color = color) +
    scale_x_continuous(labels = scales::dollar) +
    theme_tq() +
    scale_size(range = c(3, 5)) +
    theme(legend.position = "none")
  
  
  if(include_lbl){
    g <- g + 
    geom_label(
      aes_string(label = "value_text", size = value_name),
      hjust = "inward",
      color = color
    )
  } 
  
  return(g)

}


```

It looks like **Sales Executive** and **Laboratory Technician** are biggest sales driver of cost


```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

dept_job_role_tbl %>% 
  
  count(JobRole, Attrition) %>% 
  count_to_pct(JobRole)%>%
  assess_attrition(Attrition, attrition_value = "Yes", baseline_pct = 0.088) %>% 
  mutate(cost_of_attrition= calculate_attrition_cost(n=n,80000)
  ) %>% 
  
  plot_attrition(JobRole, .value = cost_of_attrition, units= "M") +
  labs(title = "Estimated Cost of Attrition By Job Role",
       x =  "Cost of Attrition" ,
       y="",
       subtitle = "Looks like Sales Executive and Laboratory Technician are biggest sales driver of cost")


```





## Phase 2 : Data Understanding

**Loading datasets**

```{r, message=FALSE, warning=FALSE}
#Load Data
path_train <- "00_Data/telco_train.xlsx"
path_test <- "00_Data/telco_test.xlsx"

path_data_definitions <- "00_Data/telco_data_definitions.xlsx"

train_raw_tbl <- read_excel(path_train, sheet=1)
test_raw_tbl <- read_excel(path_test, sheet=1)

definitions_raw_tbl <- read_excel(path_data_definitions, sheet = 1, col_names = FALSE)
```

**Reading data definitions table**

```{r, message=FALSE, warning=FALSE}
definitions_raw_tbl 
```

**Reading training dataset**

```{r, message=FALSE, warning=FALSE}
train_raw_tbl %>% glimpse()

```

**Reading test dataset**

```{r, message=FALSE, warning=FALSE}
test_raw_tbl  %>% glimpse()
```

### Exploratory Data Analysis  

#### Step 1 : Data Summarisation  

**Summarising data with skim**  

It allows you to skim useful summary statistics in console or use those statitics in a pipeable workflow  

```{r, message=FALSE, warning=FALSE}
skim(train_raw_tbl)
```

Separating your data by data type is a great way to investigate properties of data :   

**1. Character / Categorical**  
**2. Numeric**

**Exploring features which are of Character Data Type**  
```{r, message=FALSE, warning=FALSE}
# Character Data Type
train_raw_tbl %>% 
  select_if(is.character) %>% 
  glimpse()
```

Unique values in each character type feature and proportion of values in it  
```{r, message=FALSE, warning=FALSE}

train_raw_tbl %>% 
  select_if(is.character) %>% 
  map(~ table(.) %>% prop.table())
```

**Exploring features which are of Numeric Data Type**  
```{r, message=FALSE, warning=FALSE}

# Numeric Data Type

train_raw_tbl %>% 
  select_if(is.numeric) %>% 
  map(~ unique(.) %>% length())

```

We should note that numeric varibales that are lower in levels are likely to be discrete 
and that are higher in levels are likely to be continuous, filtering features where levels are less than 10, those features are likely to be factor.

```{r, message=FALSE, warning=FALSE}
train_raw_tbl %>% 
  select_if(is.numeric) %>% 
  map_df(~ unique(.) %>% length()) %>%
  gather() %>% 
  arrange(value) %>%
  filter(value <= 10)
```


#### Step 2 : Data Visualization

Visualising the feature-target interactions with [GGally](http://ggobi.github.io/ggally/) package


```{r, message=FALSE, warning=FALSE}
plot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {
  
  color_expr <- enquo(color)
  
  if (rlang::quo_is_null(color_expr)) {
    
    g <- data %>%
      ggpairs(lower = "blank") 
    
  } else {
    
    color_name <- quo_name(color_expr)
    
    g <- data %>%
      ggpairs(mapping = aes_string(color = color_name), 
              lower = "blank", legend = 1,
              diag = list(continuous = wrap("densityDiag", 
                                            alpha = density_alpha))) +
      theme(legend.position = "bottom")
  }
  
  return(g)
  
}
```

**Exploring features Visually by Category** 

Based on intuition we can divide the features into different categories, which would helps us doing a better analysis.

**1. Descriptive Features : Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}


train_raw_tbl %>% 
  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>% 
  plot_ggpairs(color = Attrition)
```

* It looks like younger people are leaving the company more so than older people.  
* It seems like people who are living between 20-30 miles away from company are leaving  
as compared to people who are within 10 miles stay at company (skewed distribution).  
* It looks like there is an upward trend of attrition (Yes) in marital status in Single, Married and Divorced while on the other side its small, large and medium.  

**2. Employment Features : Department, JobRole , JobLevel, JobInvolvement, JobSatisfaction**  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

train_raw_tbl %>% 
  select(Attrition, contains("employee"), contains("department"), contains("job") ) %>% 
  plot_ggpairs(color = Attrition)
```

* Employee count won't add any value to predictive power of model.    
* Also, Employee number won't help but it would help in identifying a particular employee.    
* If you look at Department wise R & D has higher attrition, then Sales and then HR.    
* On the contrary, density for JobInvolvement type 3 has a higher spike for people who are 
staying with relation to people who are leaving or conversely JobInvolvement type 3 
has less population as compared to lower levels. So, people in 1 and 2 type are more likely to 
leave as compared to  type 3 & 4.    
* There are lot more people leaving in Joblevel 1 and lot more people are staying who are in Joblevel 2nd
* We observe that certain JobRoles has higher attrition rate.    
* It looks like people who are at Jobsatisfaction level 1 are more likely to leave than others.  

**3. Compensation Features : DailyRate , HourlyRate, MonthlyRate, MonthlyIncome, PercentSalaryHike, StockOptionLevel**  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

train_raw_tbl %>% 
  select(Attrition, contains("income"), contains("rate"), contains("salary"), contains("stock") ) %>% 
  plot_ggpairs(color = Attrition)
```

* Persons who have less MonthlyIncome are more likely to leave.  
* Persons who have less DailyRate  are more likely to leave.  
* It looks like people with less percent salary hike are more likely to leave.  
* We observe that people with StockOptionLevel 0 i.e no stocks are more likely to leave than others.  

**4. Survey Results : EnvironmentSatisfaction, JobSatisfaction , RelationshipSatisfaction, WorkLifeBalance** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

train_raw_tbl %>% 
  select(Attrition, contains("satisfaction"), contains("life") ) %>% 
  plot_ggpairs(color = Attrition)
```

* It looks like density of environment satisfaction 1 has higher spike for attrition
  i.e those people are more likely to leave.    
* People with less JobSatisfaction are more likely to leave.  
* People in RelationshipSatisfaction level 1 are more likely to leave than the others.  
* It seems like density for WorkLifeBalance type 2 & 3 has a higher spike for people who are 
staying with relation to people who are leaving or conversely WorkLifeBalance  type 1 & 4 
has less population as compared to lower levels. So, people in 1 and 4 type are more likely to 
leave as compared to  type 2 & 3.    

**5. Performance Data : JobInvolvement, PerformanceRating**  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

train_raw_tbl %>% 
  select(Attrition, contains("performance"), contains("involvement")) %>% 
  plot_ggpairs(color = Attrition)
```

* People either get a performance rating of either 3 and 4.  
* It looks like average performance rating at the company is 3.  
* People are more likely to stay in both the cases or less people leave who have a rating of 4 (less population with rating 4).    

**6. Work - Life Features : BusinessTravel , OverTime** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

train_raw_tbl %>% 
  select(Attrition, contains("overtime"), contains("travel")) %>% 
  plot_ggpairs(color = Attrition)
```

* People who work overtime are more likely to leave as compared to those who don't.  
* People who business travel frequently and rarely are more likely to leave.  

**7. Training & Education : Education, EducationField , TrainingTimesLastYear** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

train_raw_tbl %>% 
  select(Attrition, contains("training"), contains("education")) %>% 
  plot_ggpairs(color = Attrition)
```

* People with TrainingTimesLastYear > 0 are more likely to stay than others .Also, in TrainingTimesLastYear type 4 are more likely to leave.  
* People who comes from LifeSciences, Medical, Technical Degree & Marketing are more likely to leave the company.    
* People with Education level 4 and 5 are more likely to leave.  

**8. Time - Based Features: TotalWorkingYears , YearsAtCompany , YearsInCurrentRole , YearsSinceLastPromotion, YearsWithCurrManager** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}

train_raw_tbl %>% 
  select(Attrition, contains("years")) %>% 
  plot_ggpairs(color = Attrition)

```

* People with greater TotalWorkingYears are more likely to stay.    
* People who have less YearsAtCompany are more likely to leave.  
* It looks like people with  YearsInCurrentRole has a skewed distribution although people with less YearsInCurrentRole are more likely to stay.    
* People with more YearsSinceLastPromotion are more likely to leave.  
* People whose YearsWithCurrManager are within 2 years are more likely to leave.    

## Phase 3 : Data Preparation

### Data Preparation - Human Readable

Function **process_hr_data_readable** fills up empty values in data definition table and re-levels features like Business Travel and Marital Status in correct order.  

```{r, message=FALSE, warning=FALSE}

process_hr_data_readable <- function(data, definitions_tbl) {
  
definitions_list <- definitions_tbl %>%
    fill(X__1, .direction = "down") %>%
    filter(!is.na(X__2)) %>%
    separate(X__2,into = c("key", "value"),sep = " '",remove = TRUE) %>%
    rename(column_name = X__1) %>%
    mutate(key = as.numeric(key)) %>%
    mutate(value = value %>% str_replace(pattern = "'", replacement = "")) %>%
    split(.$column_name) %>%
    map( ~ select(.,-column_name)) %>%
    map( ~ mutate(., value = as_factor(value)))
  
  for (i in seq_along(definitions_list)) {
    list_name <- names(definitions_list)[i]
    colnames(definitions_list[[i]]) <-c(list_name, paste0(list_name, "_value"))
  }
  
  data_merged_tbl <- list(HR_Data = data) %>%
    append(definitions_list, after = 1) %>%
    reduce(left_join) %>%
    select(-one_of(names(definitions_list))) %>%
    set_names(str_replace_all(names(.), pattern = "_value", replacement = "")) %>%
    select(sort(names(.))) %>%
    mutate_if(is.character, as.factor) %>%
    mutate(
      BusinessTravel = BusinessTravel %>% fct_relevel("Non-Travel", "Travel_Rarely", "Travel_Frequently"),
      MaritalStatus  = MaritalStatus %>% fct_relevel("Single", "Married", "Divorced")
    )
  
  return(data_merged_tbl)
  
} 

process_hr_data_readable(train_raw_tbl,definitions_tbl = definitions_raw_tbl) %>%
  glimpse()

```

### Data Preparation - Machine Readable With Recipes  

**Recipes:**  

* Alternative Method for creating and pre-processing design matrices that can be used for modeling
or visualization.  
* Creates a template assigning roles to variables within data.
* Formula sets the stage for subsequent pre-processing steps.  
* Formula creates the outcomes and predictor roles, assigning each variable or feature to a role  
* Features are transformed into a format that can be digested by ML algorithms.  

**Faceted Histogram Plotting Function**  

* It would help us in inspecting feature distributions to check **Skewness**.
* Identify transformations needed to get them properly formatted for **Correlation Analysis**.


```{r, message=FALSE, warning=FALSE}

train_readable_tbl <- process_hr_data_readable(train_raw_tbl, definitions_tbl = definitions_raw_tbl)
test_readable_tbl <- process_hr_data_readable(test_raw_tbl, definitions_tbl = definitions_raw_tbl)


plot_hist_facet <- function(data, bins = 10, ncol = 5, 
                            fct_reorder = FALSE, fct_rev = FALSE,
                            fill = palette_light()[[3]],
                            color = "white", scale = "free") {
  
  data_factored <- data %>%
    mutate_if(is.character, as.factor) %>%
    mutate_if(is.factor, as.numeric) %>%
    gather(key = key, value = value, factor_key = TRUE)
  
  if(fct_reorder) {
    data_factored <- data_factored %>%
      mutate(key = as.character(key) %>% as.factor())
  }
  
  if(fct_rev){
    data_factored <- data_factored %>%
      mutate(key = fct_rev(key))
  }
  
  g <- data_factored %>%
    ggplot(aes(x = value, group = key )) +
    geom_histogram(bins = bins, fill = fill , color = color) +
    facet_wrap(~ key, ncol = ncol, scale = scale) +
    theme_tq()

  
  return(g)
  
}


#Removing zero variance features for facet histogram 
zeroVar <- function(data, useNA = 'ifany') {
  out <- apply(data, 2, function(x) {length(table(x, useNA = useNA))})
  which(out==1)
}

zv <- names(zeroVar(train_raw_tbl))

```

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}
train_raw_tbl %>%
  select(-zv) %>%
  select(Attrition, everything()) %>%
  plot_hist_facet(bins = 10, ncol = 5, fct_rev = F)
```

* Features like YearsSinceLastPromotion, YearsAtCompany, MonthlyIncome, TotalWorkingYears, NumCompaniesWorked, DistanceFromHome, YearsInCurrentRole, PercentSalaryHike, YearsWithCurrManager are skewed features, we might want to inspect them for transformations.

* Features like EmployeeCount, Over18, Standard Hours have same value i.e zero variance, so its a good idea to not consider these features in analysis.  

* Features like Gender, JobLevel, JobSatisfaction, StockOptionLevel are factors since histograms works with numeric features we converted them to numeric variables for this plot.    

**Data Preprocessing with Recipes**

**Recipes are generally a 3-part process**

* Create the instructions with recipes and steps  
* Prepare the recipe  
* Bake the new data  

There are quite a few steps available, following guidelines might be useful in creating a generic checklist for pre-processing data for ML :  

* Impute - Act of filling in missing values within features. Common methods include  recency (tidyr::fill),    similarity (knn impute)  
* Individual transformation for skewness and other issues  
* Discretize (if needed and if you have no other choice) i.e making continuous variable discrete. For ex,       think of turning a variable like age into cohorts of less than 20 years, 20-30, 30-40 etc.  
* Create dummy variable i.e turning categorical data into separate columns of zeros and ones. This is           important for machine learning to detect patterns in unordered data.    
* Data Normalization steps (centering, scaling, range) . Normalization is getting the data onto a consistent    scale. Some machine learning algorithms (e.g PCA, KNN, Deep learning etc) depend on the data to all have      same scale
* Multivariate Transformation (e.g PCA, spatial sign etc...)  

**Note**: Discretization can hurt correlations. It's often best not to discretize unless there is a specific need to do so  

**1. Zero Variance Features** 

* Features that have no variance, and therefore lend nothing the predictive quality of model    
* Remove attributes with a zero variance (i.e all the same value)  

```{r, message=FALSE, warning=FALSE}

recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors())

recipe_obj


recipe_obj %>%
  prep() %>%
  bake(new_data = train_readable_tbl)

```

**2. Data Transformations** 

* Changes the data to remove skewness (e.g log), stabilize variance (e.g Box Cox) or make stationary (e.g difference for time series)  
* Computing the skewness of numeric features and filtering which are highly skewed 

```{r, message=FALSE, warning=FALSE}

skewed_feature_names <- train_readable_tbl %>%
  select_if(is.numeric) %>%
  map_df(skewness) %>%
  gather(factor_key = TRUE) %>%
  arrange(desc(value)) %>%
  filter(value >= 0.8) %>%
  pull(key) %>%
  as.character()

# Printing skewed feature names 
skewed_feature_names

```

Plotting skewed features to identify which needs to be converted into factors  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}

train_readable_tbl %>%
  select(skewed_feature_names) %>%
  plot_hist_facet()

```

Based on plot we observe that **Joblevel** and **StockOptionLevel** are not numeric. These features needs to be converted to factors and should not be transformed

```{r, message=FALSE, warning=FALSE}

skewed_feature_names <- train_readable_tbl %>%
  select_if(is.numeric) %>%
  map_df(skewness) %>%
  gather(factor_key = TRUE) %>%
  arrange(desc(value)) %>%
  filter(value >= 0.8) %>%
  filter(!key  %in% c("JobLevel", "StockOptionLevel")) %>%
  pull(key) %>%
  as.character()

skewed_feature_names
```

**Yeo-Johnson Transform**

Applying a Yeo-Johnson transform, like a BoxCox,  but values can be negative

```{r, message=FALSE, warning=FALSE}

factor_names <-  c("JobLevel", "StockOptionLevel")

recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_YeoJohnson(skewed_feature_names) %>%
  step_num2factor(factor_names)
```

**Faceted Histogram for Skewed features**  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}

recipe_obj %>%
  prep() %>%
  bake(train_readable_tbl) %>%
  select(skewed_feature_names) %>%
  plot_hist_facet()
    
```

**3. Data Center/Scaling**

Converting numeric data into different scales to be on same scale since some algorithm requires feature scaling e.g Kmeans, Deep learning, PCA & SVMs. Its hard to remember which algorithm needs it, it's better to center and scale. This 
typically don't hurt the predictions.

**Note** : Always center before you scale

```{r, message=FALSE, warning=FALSE}

recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_YeoJohnson(skewed_feature_names) %>%
  step_num2factor(factor_names) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric())

# Before preparing Recipes, mean is NULL
recipe_obj$steps[[4]] 

# After preparing recipes, mean is not null 
prepared_recipe <- recipe_obj %>% prep()
prepared_recipe$steps[[4]]
```

  
**Plotting Features After Transformation**

We observe that features are no more skewed now

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}

prepared_recipe %>%
  bake(new_data = train_readable_tbl) %>%
  select_if(is.numeric) %>%
  plot_hist_facet()

```

**4. Dummy Variables**

Expanding categorical features into multiple columns of 0's and 1's. This is important for machine learning algorithms to detect patterns in unordered data

**Categorical feature before expanding**

We will be looking at histogram of feature which contains **JobRole** in its name. We observe that it is just one
feature before any transformation  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}

# Before expanding

recipe_obj %>%
  prep() %>%
  bake(new_data = train_readable_tbl) %>%
  select(contains("JobRole")) %>%
  plot_hist_facet()
``` 

**Categorical feature after expanding**

* We are using step_dummy which will convert character/factors into one or more numeric terms 
* Generally, if factor has 3 levels, the feature is expanded into 2 columns (1 less than the number of levels)
* We will obtain the full set of dummy variables using `one_hot` argument 

```{r, message=FALSE, warning=FALSE,  fig.width = 16, fig.height = 7}

# After expanding  

dummied_recipe_obj  <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_YeoJohnson(skewed_feature_names) %>%
  step_num2factor(factor_names) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(), one_hot = TRUE)

dummied_recipe_obj %>%
  prep() %>%
  bake(new_data = train_readable_tbl) %>%
  select(contains("JobRole")) %>%
  plot_hist_facet(ncol = 3)
```

We notice that feature **JobRole** now has been expanded to following 9 features :   

* JobRole_Healthcare.Representative  
* JobRole_Human.Resources  
* JobRole_Laboratory.Technician  
* JobRole_Manager  
* JobRole_Manufacturing.Director  
* JobRole_Research.Director  
* JobRole_Research.Scientist  
* JobRole_Sales.Executive  
* JobRole_Sales.Representative  

**Final Recipe for Correlation Evaluation**

```{r, message=FALSE, warning=FALSE}

recipe_obj  <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_YeoJohnson(skewed_feature_names) %>%
  step_num2factor(factor_names) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(),one_hot = TRUE) %>%
  prep()

# Printing Recipe
recipe_obj
```

**Training data after applying recipe**  

```{r, message=FALSE, warning=FALSE}

train_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
train_tbl %>% glimpse()
```

**Test data after applying recipe**  

```{r, message=FALSE, warning=FALSE}

test_tbl <- bake(recipe_obj, new_data = test_readable_tbl)
test_tbl %>% glimpse()
```

### Correlation Analysis

Correlation Analysis is a great way to determine if you are getting good features prior to modeling. It is always a good idea to spend your time on collecting data on good features. If you do not have any feature exhibiting low correlation, get different data, else analysis isn't going to look good.  

Features exhibiting high correlation, we should immediately report them as potential area of focus. Early detection helps stakeholders building strategies.

 
Function **get_cor** takes in data and measure correlation against targeted feature.
 
```{r, message=FALSE, warning=FALSE}

# fct_reorder: reorders a factor by another column. It changes the level of factors but does not rearrange the data frame. We do this factor level reordering for plotting function, plot_cor()

get_cor <- function(data, target, use = "pairwise.complete.obs",
                     fct_reorder = FALSE, fct_rev = FALSE){
    
   feature_expr <- enquo(target)
  feature_name <- quo_name(feature_expr)
  
  data_cor <-  data %>%
    mutate_if(is.character, as.factor) %>%
    mutate_if(is.factor, as.numeric) %>%
    cor(use = use) %>%
    as.tibble() %>%
    mutate(feature = names(.)) %>%
    select(feature, !! feature_expr) %>%
    filter(!(feature == feature_name)) %>%
    mutate_if(is.character, as_factor)

  if(fct_reorder){
    data_cor <- data_cor %>%
      mutate(feature = fct_reorder(feature, !! feature_expr)) %>%
      arrange(feature)
  }
  
  if(fct_rev){
    data_cor <- data_cor %>%
      mutate(feature = fct_rev(feature)) %>%
      arrange(feature)
  }
  
return(data_cor)  
}

```

Using **train_tbl** which has been well formatted for Correlation Analysis

```{r, message=FALSE, warning=FALSE}

train_tbl %>% 
  get_cor(Attrition_Yes, fct_reorder = T, fct_rev = T )

```

Function **plot_cor** takes in data and plot correlation against targeted feature i.e Attrition in this case.
```{r, message=FALSE, warning=FALSE}

data         <- train_tbl
feature_expr <- quo(Attrition_Yes)

plot_cor <- function(data, target, fct_reorder = FALSE, fct_rev = FALSE,
                     include_lbl = TRUE, lbl_precision = 2, lbl_position = "outward",
                     size = 2, line_size = 1, vert_size = 1,
                     color_pos = palette_light()[[1]],
                     color_neg = palette_light()[[2]]){

  
  feature_expr <- enquo(target)
  feature_name <- quo_name(feature_expr)

  data_cor <-  data %>%
      get_cor(!! feature_expr, fct_reorder = fct_reorder , fct_rev = fct_rev) %>%
      mutate(feature_name_text = round(!! feature_expr, lbl_precision)) %>%
      mutate(Correlation = case_when(
        (!! feature_expr) >=0 ~ "Positive",
        TRUE                  ~ "Negative") %>% as.factor())
  

      g <- data_cor %>%
        ggplot(aes_string(x = feature_name, y = "feature", group = "feature")) +
        geom_point(aes(color = Correlation), size = size) +
        geom_segment(aes(xend = 0, yend = feature, color = Correlation), size = line_size) +
        geom_vline(xintercept = 0, color = palette_light()[[1]], size = vert_size) +
        expand_limits(x = c(-1,1)) +
        theme_tq() +
        scale_color_manual(values = c(color_neg, color_pos))
      
      
    if(include_lbl) g <- g + geom_label(aes(label = feature_name_text), hjust = lbl_position)
    
    return(g)    
}

```

**Correlation Evaluation : After Transformation**

Variables that contribute most to attrition lies at the top


**Explore features by Category** 

**1. Descriptive Features : Age, Gender, MaritalStatus** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}
train_tbl %>% 
  select(Attrition_Yes, Age, contains("Gender"), contains("MaritalStatus"), 
         NumCompaniesWorked, contains("Over18"), DistanceFromHome) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

In this section, we will see how descriptive features correlate with Attrition: 

* As Age increases, person has lower probability of leaving the company (negative correlation is good in this case) compared to a person who are in their early 20's.    
* There is a positive correlation between  DistanceFromHome and Attrition which means it has a effect on Attrition. For instance, as the distance from home increases from company, there are more chances that employee may leave.    

**2. Employment Features : Department, JobRole , JobLevel**  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}
train_tbl %>% 
  select(Attrition_Yes, contains("employee"), contains("department"), contains("job") ) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

We can say that there is a relationship between Attrition and Joblevel_X1. So, its a potential candidate for feature of interest. 

**3. Compensation Features : DailyRate ,HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}
train_tbl %>% 
  select(Attrition_Yes, contains("income"), contains("rate"), contains("salary"), contains("stock") ) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

In this case, we observe that people who have StockOptionLevel_X0 have a effect on Attrition. So, they are more likely to leave the company.  

**4. Survey Results : EnvironmentSatisfaction, JobSatisfaction , RelationshipSatisfaction, WorkLifeBalance** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}
train_tbl %>% 
  select(Attrition_Yes, contains("satisfaction"), contains("life") ) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

We can say that people who have low environment satisfaction level at company and bad worklife balance are more likely to leave the company since they exhibit a positive correlation with the Attrition which is not much significant but for sure has an effect on Attrition.  

**5. Performance Data : JobInvolvement, PerformanceRating**  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}
train_tbl %>% 
  select(Attrition_Yes, contains("performance"), contains("involvement")) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

People whose JobInvolvement is low are more likely to leave the company.

**6. Work - Life Features : BusinessTravel , OverTime** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}
train_tbl %>% 
  select(Attrition_Yes, contains("overtime"), contains("travel")) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

* People who work OverTime has a strong effect on Attrition as compared to people who don't. They are more likely to leave the company  
* People who travel for business frequently have a positive relationship with Attrition. So, they are more likely to leave the company. 

**7. Training & Education : Education, EducationField , TrainingTimesLastYear** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}
train_tbl %>% 
  select(Attrition_Yes, contains("training"), contains("education")) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

* We can say that people who have education field as Technical Degree, Human Resources & Marketing are more likley to leave the company  

**8. Time - Based Features: TotalWorkingYears , YearsAtCompany , YearsInCurrentRole , YearsSinceLastPromotion, YearsWithCurrManager** 

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}
train_tbl %>% 
  select(Attrition_Yes, contains("years")) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)

```

We can say that all the time based features have a negative correlation with Attrition which is good in our case.

## Phase 4 : Modeling Churn with H2Os AutoML

In this phase, we will see how we turn information into business insights ?

### Encode Decision Making Algorithms

**Step 1: Develop algorithms to predict target and explain in terms of business levers**

**First step is to select the technique:** 

* Prediction
* Classification
* Regression
* Clustering
* Anomaly Detection 

In this case, it is a **Binary Classification problem**

* Employees that are likely to leave are coded as 0  
* Employees that are likely to leave are coded as 1  


**ML Preprocessing with Recipes**

We will cut down some steps in our final recipe before automl since h2o is a low maintenance algorithm. It handles factor and numeric data and performs pre-processing internally and transformations are not necessary.


```{r, message=FALSE}

recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_num2factor(JobLevel, StockOptionLevel) %>%
  prep()

recipe_obj

train_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
test_tbl <- bake(recipe_obj, new_data = test_readable_tbl)

```

**Initialising H2O**

```{r, message=FALSE}

h2o.init()

# Splitting the data into train/valid/test and converting tibble to h2o data frame
split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)

train_h2o <- split_h2o[[1]]
valid_h2o <- split_h2o[[2]]
test_h2o <- as.h2o(test_tbl)


# Defining outcome / response variable
y <- "Attrition"

# Defining explanatory variables
x <- setdiff(names(train_h2o),y)
```

**Running AutoML**

* *Training Frame*   : Used to develop model  
* *Validation Frame* : Used to tune hyperparameters via grid search  
* *Leaderboard Frame*: Test set completely held out from model training & tuning  
* Use *max_run_time_secs* to minimize modeling time. Once results look promising we can always increase the run time to get more models with highly tuned parameters.  


```{r, message=FALSE, eval = FALSE}

automl_models_h2o <- h2o.automl(
    x = x,
    y = y,
    training_frame = train_h2o,
    validation_frame = valid_h2o,
    leaderboard_frame = test_h2o,
    max_runtime_secs = 30,
    nfolds = 5
)

```

Producing Summary of models produced by H2O

```{r, message=FALSE, eval = FALSE}

automl_models_h2o@leaderboard

```

In the AutoML leaderboard, we get the models ordered with **highest AUC** and **minimial Logloss**

**Saving and Loading H2O Models**

```{r, message=FALSE, eval= FALSE}

h2o.getModel("DeepLearning_1_AutoML_20190117_191950") %>%
  h2o.saveModel(path = "04_Modeling/h20_models/")

h2o.getModel("GLM_grid_1_AutoML_20190117_191950_model_1") %>%
  h2o.saveModel(path = "04_Modeling/h20_models/")

h2o.getModel("StackedEnsemble_AllModels_AutoML_20190117_191950") %>%
  h2o.saveModel(path = "04_Modeling/h20_models/")

deeplearning_h2o <- h2o.loadModel("04_Modeling/h20_models/DeepLearning_1_AutoML_20190117_191950")

glm_h2o <- h2o.loadModel("04_Modeling/h20_models/GLM_grid_1_AutoML_20190117_191950_model_1") 

stacked_ensemble_h2o <- h2o.loadModel("04_Modeling/h20_models/StackedEnsemble_AllModels_AutoML_20190117_191950")

```

In our case, We will focus more on exploring following leader models :

* *GLM*
* *Stacked Ensemble*
* *Deep Learning*

Produces leader model in the summary by H2O

```{r, message=FALSE}

automl_models_h2o@leader

```

**Making Predictions**  

```{r, message=FALSE, eval= FALSE}
predictions <- h2o.predict(deeplearning_h2o, newdata = as.h2o(test_tbl))
```

```{r, message=FALSE}

# Saving it as a tibble
predictions_tbl <- predictions %>% as.tibble()
predictions_tbl
```

**Visualizing Leaderboard**

**Why is there need to Visualize leaderboard ?**

Answer to that is **Model Comparison** . We observe that in leaderboard we get the model with **highest AUC** and **minimial Logloss** but minimal logloss  may not always be lowest for highest AUC model. So, it's a good idea to visualize these numbers which we will do later

* Here, idea is to see how the models are performing in comparison to each other  
* We can also see if the model position gets altered since we have added model position number  
* Also, each model type is represented with a different color in plot  

Function **plot_h2o_leaderboard ** takes in H2O leaderboard and measure correlation against targeted feature

```{r, message=FALSE, warning=FALSE}

plot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c("auc", "logloss"),
                                 n_max = 20, size = 4, include_lbl = TRUE) {
  
  #Setup input 
  order_by <- tolower(order_by[[1]])
  
  leaderboard_tbl <- h2o_leaderboard %>%
    as.tibble() %>%
    select(model_id,auc,logloss) %>%
    mutate(model_type = str_split(model_id, '_', simplify = TRUE)[,1]) %>%
    rownames_to_column(var = "rowname") %>%
    mutate(model_id = paste0(rowname, ". ",as.character(model_id)) %>% as_factor())
  
  
  # Transformation 
  if(order_by == "auc" ) {
    
    data_transformed_tbl <- leaderboard_tbl %>%
      slice(1:n_max) %>%
      mutate(
        model_id = as_factor(model_id) %>% reorder(auc),
        model_type = as_factor(model_type)
      ) %>%
      gather(key = key, value = value, 
             -c(model_id, model_type, rowname), factor_key = T) 
  } 
  else if(order_by == "logloss" ){
    data_transformed_tbl <- leaderboard_tbl %>%
      slice(1:n_max) %>%
      mutate(
        model_id = as_factor(model_id) %>% reorder(logloss)%>% fct_rev(),
        model_type = as_factor(model_type)
      ) %>%
      gather(key = key, value = value, 
             -c(model_id, model_type, rowname), factor_key = T)
  } else{
    stop(paste0(" order_by = '", order_by,"' is not a permitted option. "))
  }
  
 g <- data_transformed_tbl %>%
        ggplot(aes(value, model_id, color = model_type)) +
        geom_point(size = size) +
        facet_wrap(~ key, scales = "free_x") +
        theme_tq() +
        scale_color_tq() +
        labs(title = "Leaderboard Metrics",
             subtitle = paste0("Ordered by: ", toupper(order_by)),
             y = "Model Postion, Model ID", x = "")
  
  if(include_lbl) g <- g +  geom_label(aes(label  = round(value, 2), hjust = "inward")) 
  
  return(g)
  
}  

```


**H2O LeaderBoard Performance : Ordered by AUC**

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

automl_models_h2o@leaderboard %>%
  plot_h2o_leaderboard(order_by = "auc")

```

We can see that first 3 models have much better performance than others w.r.t highest auc and minimal logloss


**H2O LeaderBoard Performance : Ordered by LogLoss**

* We observe that now model positions have been changed Stacked Ensemble models are on top now then GLM & Deep Learing Model  
* After that we see a steep drop between auc and logloss, we might not need to inspect those models further  


```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

automl_models_h2o@leaderboard %>%
  plot_h2o_leaderboard(order_by = "logloss")

```

**Assessing H2O Performance**  

**h2o.performance()** : Creates an H2O performance object by providing model and newdata


```{r, message=FALSE, warning=FALSE}
# Assessing Performance 
performance_h2o <- h2o.performance(stacked_ensemble_h2o, newdata =  as.h2o(test_tbl))

# prints all the classifier metrics 
performance_h2o@metrics

```

We will now see how to extract some of the above metrics individually using *performance object*

**Classifier Summary metrics** 

**Area Under Curve**  
It's a way of measuring the performance of a binary classifier by comparing the 
False Positive Rate (FPR x-axis) to True positive rate (TPR y-axis)  

```{r, message=FALSE, warning=FALSE}
# Retreives area under curve (AUC) for a classifier. If passing an h2O Model, can use the xval argument
# to retreive the average cross validation AUC
h2o.auc(performance_h2o)

```
We can see that accuracy is 86.1% on test data. 

Arguments train, valid and xval only works for models, not for performance objects. 
```{r, message=FALSE, warning=FALSE}

h2o.auc(stacked_ensemble_h2o, train = TRUE, valid =  TRUE, xval = TRUE)

```

This model has 96% accuracy on training set, 82% accuracy on validation set & 83% accuracy on cross validation set.  

**Gini Coefficient**  

* Gini coefficient is a statistic which measures the ability of a scorecard or a characteristic to rank order risk.    
* A Gini value of 0% means that the characteristic cannot distinguish good from bad cases. In general, Gini coeff above 60% is a good model.    
* AUC = (GiniCoeff + 1) / 2  

```{r, message=FALSE, warning=FALSE}

h2o.giniCoef(performance_h2o)
```

We have GiniCoeff of 0.72 which implies it is a good model 

**Logloss**  

* Log Loss quantifies the accuracy of a classifier by penalising false classifications.     
* Great way to Measure the true performance of classifier by comparing the class probability to actual value (1 or 0).  
* Minimising log loss gives greater accuracy for the classifier.   

```{r, message=FALSE, warning=FALSE}

h2o.logloss(performance_h2o)
```

**Confusion Matrix**  

* Used for summarizing the performance of a classification algorithm. 
* It shows the ways in which your classification model is confused when making predictions.
* Critical for *Business Analysis*
* Confusion matrix varies based on threshold ( value that determines which class probability is 1 or 0). 

```{r, message=FALSE, warning=FALSE}

h2o.confusionMatrix(stacked_ensemble_h2o)
```

```{r, message=FALSE, warning=FALSE}

h2o.confusionMatrix(performance_h2o)
```

We need to be able to undestand how these model change with thresholds using **h2o.metric()** (Converts a performance object into a series of metrics (e.g precision, recall, f1 etc) that vary by threshold)   

**Important measures that vary by threshold :** 

**Precision**  

* Measures false positives (e.g predicted to leave but actually stayed )  
* Precision = TP / (TP +FP)  # 24 /(24+14) = 0.63
* In other words, it detects how frequently your algorithm over-picks the Yes class    
* Precision in business context: Precision indicates how often we incorrectly say people will leave
  when they actually will stay  

**Recall**  

* Measures false negatives (e.g predicted to stay but actually left)  
* Recall =  TP / (TP + FN) # 24 /(24+12) = 0.66
* In other words, it provides a metric for under-picking the Yes class  
* Recall is typically more important than Precision in the business context. We would rather give
  up some false positives (inadeverently target stayers) to gain false negatives (accurately predict quiters) * Recall in business context: Recall indicates how often we miss people that will leave by incorrectly
  predicting they will stay  


**F1**  

* Optimal balance between precision and recall. Typically the threshold that maximises F1 is used as threshold   cutoff for turning class probability into 0/1. However this is not always the best case, An expected value    optimization is required when costs of false positives and false negatives are known  
* F1 = 2 (precision x recall) / (precision + recall) #  2 x (0.63 x 0.66) / (0.63+0.66) = 0.64  
* In other words, it provides a metric for balancing  precision vs recall  
* Max F1 Thresholds: Threshold that optimizes the balance between precision and recall  

**More Important Measures**  

* True positives (tps), True negatives (tns), False positives (fps), and False negatives (fns) are
often converted into rates to understand cost  / benefit of classifier  
* Rates are included as tpr, tnr, fpr and fnr  

```{r, message=FALSE, warning=FALSE}
performance_h2o %>%
# h2o.metric(): Returns the classifier performance
  h2o.metric() %>%
  as.tibble() %>%
  glimpse()
```

If we vary the threshold, metrics completely changed and if we do that over 1 to 0 we can see how that 
affects the model across various metrics.  

**Gain & Lift**  

Emphasizes how much model improves results  

**Ranked Predictions :** By ranking by class probability of Yes, we assess the models ability to truly detect someone that is leaving
```{r, message=FALSE, warning=FALSE}

ranked_predictions_tbl <- predictions_tbl %>%
  bind_cols(test_tbl) %>%
  select(predict:Yes, Attrition) %>%
  arrange(desc(Yes))

ranked_predictions_tbl
```

* In first 10, we have 90% accuracy  (9 of 10 people)
* Without model, we'd only expect the global attrition rate for first 10 (16% or 1.6 people)
* If total expected quiters is 220 x 0.16 = 35
* Gain : If 35 people expected to quit, we gained 9 of 35 or 25.7% in first 10 cases
* Lift : If the expectation is 1.6 people, we beat the expectation by 9/1.6 = 5.6X in first 10 cases


*   **Grouping By Likelihood:** Grouping into cohorts of most likely to least likely groups is as the heart of Gain/Lift chart. When we do this, we can immediately show that if candidate has high prob of leaving, 
how likely they are to leave.    

*   **ntile:** Thinking of grouping by ntile as splitting up a continuious variable into n buckets
This allows us to group the group the Response (Attrition) based on ntile column.  

* **Cumulative Gain:** Technically this is cumulative gain, because we cumulatively sum the pct_responses 

```{r, message=FALSE, warning=FALSE}

calculated_gain_lift_tbl <- ranked_predictions_tbl %>%
  mutate(ntile = ntile(Yes, n = 10)) %>%
  group_by(ntile) %>%
  summarise(
    cases = n(),
    responses = sum(Attrition == "Yes")
  ) %>%
  arrange(desc(ntile)) %>%
  mutate(group = row_number()) %>%
  select(group, cases, responses) %>%
  mutate(
    cumulative_responses = cumsum(responses),
    pct_responses        =  responses/sum(responses),
    gain                 =  cumsum(pct_responses),
    cumulative_pct_cases =  cumsum(cases)/sum(cases),
    lift                 =  gain/cumulative_pct_cases,
    gain_baseline        =  cumulative_pct_cases,
    lift_baseline        =  gain_baseline / cumulative_pct_cases
  )
```

We note that the 10th decile i.e this group has the highest class probabilities for leaving, 18 of 22 actually left.

**Gain**  

* Think of this as what we gained using the model. For example, if we focused on first 2 groups , we gain ability to target 69.4% of our quiters using our model
* Cumulative gain baseline is always equal to cumulative ntile percentage

**Lift**  

* Think of this as multiplier between what we gained divided by what we expected to gain with no model. For  example, if we focused on the first two decile groups, we gained ability to target 69.4% of our quiters but  we only expected to be able to target 20% of the quiters in 2 decile groups.  
* Cumulative lift baseline is always equal to 1.  
* Group: H2o groups into 16 ntiles with the first being the most likely to be in the minority (Yes) class 16 n-tiles  
* 220/ 16 n-tiles = 13.75 people / group  
* 16 n-tiles gives great resolution with about 14 people in each group  

Defining terms
* Cumulative Capture Rate : This is *cumulative percentage gain* that we will use  
* Cumulative Lift : This is the *cumulative lift* that we will use  

```{r, message=FALSE, warning=FALSE}

gain_lift_tbl <- performance_h2o %>%
   h2o.gainsLift() %>%
   as.tibble()

gain_lift_tbl
```

**Model Performance Comparison Dashboard**

Visualizing important model metrics and Gain & lift chart in this dashboard  

*   **ROC Plot**: Compares AUC of different models  
*   **Precision & Recall Plot**: Highly useful in comparing binary classification models  
*   **Gain & Lift Chart:**  Charts designed to communicate model performance to non-technical stakeholders    

```{r, message=FALSE, warning=FALSE}

# Performance Visualization 

plot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c("auc", "logloss"),
                                 max_models = 3, size = 1.5) {
    
    # Inputs
    
    leaderboard_tbl <- h2o_leaderboard %>%
        as.tibble() %>%
        slice(1:max_models)
    
    newdata_tbl <- newdata %>%
        as.tibble()
    
    order_by <- tolower(order_by[[1]])
    order_by_expr <- rlang::sym(order_by)
    
    h2o.no_progress()
    
    # Model metrics
    
    get_model_performance_metrics <- function(model_id, test_tbl) {
        
        model_h2o <- h2o.getModel(model_id)
        perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))
        
        perf_h2o %>%
            h2o.metric() %>%
            as.tibble() %>%
            select(threshold, tpr, fpr, precision, recall)
        
    }
    
    model_metrics_tbl <- leaderboard_tbl %>%
        mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%
        unnest() %>%
        mutate(
            model_id = as_factor(model_id) %>% 
                fct_reorder(!! order_by_expr, .desc = ifelse(order_by == "auc", TRUE, FALSE)),
            auc  = auc %>% 
                round(3) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id)),
            logloss = logloss %>% 
                round(4) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id))
        )
    
    
    # ROC Plot
    
    p1 <- model_metrics_tbl %>%
        ggplot(aes_string("fpr", "tpr", color = "model_id", linetype = order_by)) +
        geom_line(size = size) +
        theme_tq() +
        scale_color_tq() +
        labs(title = "ROC", x = "FPR", y = "TPR") +
        theme(legend.direction = "vertical")
    
    # Precision vs Recall
    
    p2 <- model_metrics_tbl %>%
        ggplot(aes_string("recall", "precision", color = "model_id", linetype = order_by)) +
        geom_line(size = size) +
        theme_tq() +
        scale_color_tq() +
        labs(title = "Precision Vs Recall", x = "Recall", y = "Precision") +
        theme(legend.position = "none")
    
    
    # Gain / Lift
    
    get_gain_lift <- function(model_id, test_tbl) {
        
        model_h2o <- h2o.getModel(model_id)
        perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
        
        perf_h2o %>%
            h2o.gainsLift() %>%
            as.tibble() %>%
            select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)
        
    }
    
    gain_lift_tbl <- leaderboard_tbl %>%
        mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%
        unnest() %>%
        mutate(
            model_id = as_factor(model_id) %>% 
                fct_reorder(!! order_by_expr, .desc = ifelse(order_by == "auc", TRUE, FALSE)),
            auc  = auc %>% 
                round(3) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id)),
            logloss = logloss %>% 
                round(4) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id))
        ) %>%
        rename(
            gain = cumulative_capture_rate,
            lift = cumulative_lift
        ) 
    
    #  Gain Plot
    
    p3 <- gain_lift_tbl %>%
        ggplot(aes_string("cumulative_data_fraction", "gain", 
                          color = "model_id", linetype = order_by)) +
        geom_line(size = size) +
        geom_segment(x = 0, y = 0, xend = 1, yend = 1, 
                     color = "black", size = size) +
        theme_tq() +
        scale_color_tq() +
        expand_limits(x = c(0, 1), y = c(0, 1)) +
        labs(title = "Gain",
             x = "Cumulative Data Fraction", y = "Gain") +
        theme(legend.position = "none")
    
    # Lift Plot
    
    p4 <- gain_lift_tbl %>%
        ggplot(aes_string("cumulative_data_fraction", "lift", 
                          color = "model_id", linetype = order_by)) +
        geom_line(size = size) +
        geom_segment(x = 0, y = 1, xend = 1, yend = 1, 
                     color = "black", size = size) +
        theme_tq() +
        scale_color_tq() +
        expand_limits(x = c(0, 1), y = c(0, 1)) +
        labs(title = "Lift",
             x = "Cumulative Data Fraction", y = "Lift") +
        theme(legend.position = "none")
    
    
    # Combine using cowplot
    p_legend <- get_legend(p1)
    p1 <- p1 + theme(legend.position = "none")
    
    p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2) 
    
    p_title <- ggdraw() + 
        draw_label("H2O Model Metrics", size = 18, fontface = "bold", 
                   colour = palette_light()[[1]])
    
    p_subtitle <- ggdraw() + 
        draw_label(glue("Ordered by {toupper(order_by)}"), size = 10,  
                   colour = palette_light()[[1]])
    
    ret <- plot_grid(p_title, p_subtitle, p, p_legend, 
                     ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))
    
    h2o.show_progress()
    
    return(ret)
    
}
```

**Model Performance Comparison Dashboard : Ordered by AUC**

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 17}

automl_models_h2o@leaderboard %>%
    plot_h2o_performance(newdata = test_tbl, order_by = "auc", 
                         size = 1, max_models = 4)

```

**Takeaways**

* We can conclude from ROC Curve that GLM model has higher AUC than Stacked Ensemble.  
* For Stacked Ensemble Best of Family models, precision drops off very quickly as recall increases which is
not desirable.    
* We can notice from Precision & Recall plot that black one i.e GLM has better recall than others
especially early one and precision begins to drop for all 3 models.  
* We can notice from *Gain Plot* that organisation can get 69% of gains by just focusing on the top 25% of employees as compared to 
baseline model where gains are only 25%.
* We notice from lift chart, we get 3x positive responses as compared to basline model which has been lifted from 1x.  
* The two charts work together to show the results of using the modeling approach versus just targeting people at random
* But all these 3 models seems to be performing very similarly.  


**Model Performance Comparison Dashboard : Ordered by Logloss**

On a similar note, we can also choose the best model based on minimal logloss

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 17}

automl_models_h2o@leaderboard %>%
    plot_h2o_performance(newdata = test_tbl, order_by = "logloss", 
                         size = 1, max_models = 4)

```

How does lift apply to Attrition ? 

* Example: Providing stock options strategically to high performers that are high risk    
* Could strategically focus on those in the high risk category that are working time  
* Stock options Assignment would be 3x more effective with the model  


**Communicating to Executive**  

* If 35 people are expected to quit, we can obtain 69% gain by targeting the top 25% of high risk people.   
* This reduces costs by 1/3rd versus random selection because we only need to offer stock options to
  high risk candidates.  

### Explaining Black-Box Classification Models With LIME

* Models that tend to be highly predictive like stacked ensembles are not interpretable by normal means which is a serious issue.  
* Explanations are critical to business 
* Key question here is why is churn happening ?  
* Knowledge empowers to make better decisions  

**LIME - Local Interpretable Model-Agnostic Explanations package**  

* Used to determine which features contribute to prediction (& by how much) for a single observation (i.e local).    
* Every complex model is linear at local level.  
* Performs localized regression 1000x and returns results which are then averaged. Features with highest importance becomes the expalanation.  
* LIME can be used to explain Black-Box classification models e.g deep learning, stacked ensembles
and random forest.    
* AutoML + LIME can be applied to any classification problem in business like customer churn,
fraud detection.    

```{r, message=FALSE, warning=FALSE}

automl_leader <- h2o.loadModel("04_Modeling/h20_models/StackedEnsemble_AllModels_AutoML_20190117_191950")

# Making Predictions

predictions_tbl <- automl_leader %>% 
  h2o.predict(newdata = as.h2o(test_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(Attrition, EmployeeNumber)
  )
```

**Steps Involved in LIME**  

1. Build explainer with lime()  
2. Create an explanation with explain()  

**Multiple Explanations**

**Using lime::explain() function to build explainer**

* Use bin_continuous to bin the features. It makes it easy to detect what causes the continuous feature to have a high feature weight explanation
* Using n_bins to tell how many bins you want
* Using quantile_bins to tell how to distribute observations with the bins. If True, cuts will be selected to evenly distribute the total observations within each of bins   

```{r, message=FALSE, warning=FALSE}
explainer <- train_tbl %>%
  select(-Attrition) %>%
  lime(
    model           =  automl_leader,
    bin_continuous  =  TRUE,
    n_bins          =  4, 
    quantile_bins   = TRUE
    )

explainer
```

Creating an explanation with explain
```{r, message=FALSE, warning=FALSE}

explanation <- test_tbl %>%
    slice(1:20) %>%
    select(-Attrition) %>%
    lime::explain(
        explainer = explainer,
        n_labels   = 1,
        n_features = 8,
        n_permutations = 5000,
        kernel_width   = 1
    )

explanation %>%
    as.tibble()

```

#### Recreating Plot Features with LIME

Function **plot_features_tq** create a visualization containing individual plot for each observations

* It plots the 8 most influential variables that best explain the linear model in that observations local region.  
* Label variable if it causes an increase in the probability as **supports** or if it causes decrease in the probability **contradicts**.  
* It provides us with the model fit labelled as Explanation fit for each model, which allows us to see how well that model explains the local region.  

```{r, message=FALSE, warning=FALSE}

plot_features_tq <- function(explanation, ncol) {
  
  data_transformed <- explanation %>%
    as.tibble() %>%
    mutate(
      feature_desc = as_factor(feature_desc) %>%
        fct_reorder(abs(feature_weight), .desc = FALSE),
      key = ifelse( feature_weight > 0, "Supports", "Contradicts") %>%
        fct_relevel("Supports"),
      case_text = glue("Case : {case}"),
      label_text = glue("Label : {label}"),
      prob_text  = glue("Probability: {round(label_prob, 2)}"),
      r2_text    = glue("Explanation Fit : {model_r2 %>% round(2)}")
    ) %>%
    select(feature_desc, feature_weight, key, case_text:r2_text)
  
  
  data_transformed %>%
    ggplot(aes(feature_desc, feature_weight, fill = key)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    theme_tq() +
    scale_fill_tq() +
    labs(y = "Weight" , x = "Feature") +
    facet_wrap(~ case_text + label_text + prob_text + r2_text, 
               ncol = ncol , scales = "free")
  
} 

```

We can infer that **Case 5** has the highest probability of leaving the company and variables that appear to be influencing this high probability include OverTime (Yes),  NumCompaniesWorked (>4), Joblevel (1), Business Travel (Frequently) & DistanceFromHome (13.8 miles).    

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}
explanation %>%
  filter(case %in% 1:6) %>%
  plot_features_tq(ncol = 2)

```


#### Recreating Plot explanation with LIME

```{r, message=FALSE, warning=FALSE}

plot_explanations_tq <- function(explanation) {
  
  
  data_transformed <- explanation %>%
    as.tibble() %>%
    mutate(
      case = as_factor(case),
      order_1 = rank(feature)
    ) %>%
    group_by(feature) %>%
    mutate(
      order_2 = rank(feature_value)
    ) %>%
    ungroup() %>%
    mutate(
      order = order_1 * 1000 + order_2
    ) %>%
    mutate(
      feature_desc  = as.factor(feature_desc) %>%
        fct_reorder(order, .desc = T)
    ) %>%
    select(case, feature_desc, feature_weight, label)
  
  
  data_transformed %>%
    ggplot(aes(case, feature_desc)) +
    geom_tile(aes(fill = feature_weight)) + 
    facet_wrap(~ label) +
    theme_tq() + 
    scale_fill_gradient2(low = palette_light()[[2]],
                         mid = "white", 
                         high = palette_light()[[1]]) +
    theme(
      panel.grid = element_blank(),
      legend.position = "right",
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)
    ) + 
    labs(y = "Feature", x = "Case", 
         fill  = glue("Feature 
                      Weight"))
  
}

```

* Heatmap showing how the different features selected across all the observations influence each case
* This plot is useful if we are trying to find common features that influence all the observations

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}  
plot_explanations_tq(explanation)
```

## Phase 5 : Evaluation

* In this phase, we try to evaluate degree to which data science insights meets the business objectives  
* Success of data science project depends upon ROI.    
* We must always focus to present a business case for making any changes, then your organization.  
decides whether or not to proceed with some or all recommendations based on ROI.  


**Expected Value Framework**  

* A way of assigning value to a decision using both the probability of its occurence and potential benefit of outcome  
* Decisions becomes based on probability rather than intuition


**Step 2:  Tie financial value of individual decisions to optimize for profit**

* *MonthlyIncome*: Used later as a way to calculate cost of policy change
* *OverTime*     : Needed to determine whether or not the employee is working overtime
* *Attrition cost* is only occurred if employee leaves. The 'Yes' column is the likelihood of leaving. When combined, we can get an expected attrition cost . 


**Calculating the Expected ROI Of a Policy Change**

**Policy I:   No OverTime for Everyone**

* "No OT Policy" i.e Threshold = 0,  anyone with over time is targeted , analysis of test set (15% of Total Population)
* **Baseline**: Do nothing results in no additional costs (e.g no cost incurred from reduction in OT) 

> Expected Cost = Yes * (Total Potential Cost) + No * (Policy change cost)

* For Baseline : Expected cost equation simplifies further since there is no policy change

> Expected Cost = Yes *  (Attrition Cost)

**Calculating Expected Value With OT**


```{r, message=FALSE, warning=FALSE}


predictions_with_OT_tbl <- automl_leader %>%
  h2o.predict(newdata = as.h2o(test_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(EmployeeNumber, MonthlyIncome, OverTime)
  )

ev_with_OT_tbl <- predictions_with_OT_tbl %>%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %>%
  mutate(
    cost_of_policy_change = 0
  ) %>%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

total_ev_with_OT_tbl <- ev_with_OT_tbl %>%
  summarise(
    total_expected_attrition_cost_0 = sum(expected_attrition_cost)
  )

```

We observed that total expected cost with Overtime policy is $3,191,435.

```{r, message=FALSE, warning=FALSE}

total_ev_with_OT_tbl

```


**Calculating Expected Value Without OT**

*   **Policy Change**: The goal is to update the calculation to reflect a new policy based on your realistic parameters you and your organisation are potentially experiencing.  
* We are interested in showing savings with no overtime policy, so we will be switching Overtime NO to Yes 
on test set and then predicting their prob again using h2o.predict().  
* Adjusting Overtime had a pretty big impact on likelihood of leaving. Let's say organization has an average overtime of 10%.  


```{r, message=FALSE, warning=FALSE}

test_without_OT_tbl <- test_tbl %>%
  mutate(OverTime = fct_recode(OverTime, "No" = "Yes"))

predictions_without_OT_tbl <- automl_leader %>%
  h2o.predict(newdata = as.h2o(test_without_OT_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(EmployeeNumber, MonthlyIncome, OverTime),
    test_without_OT_tbl %>%
      select(OverTime)
  ) %>%
  rename(
    OverTime_0 = OverTime,
    OverTime_1 = OverTime1
  )

avg_overtime_pct <- 0.10


ev_without_OT_tbl<- predictions_without_OT_tbl %>%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %>%
  mutate(
    cost_of_policy_change = case_when(
      OverTime_0 == "Yes" & OverTime_1 == "No" ~ avg_overtime_pct * attrition_cost,
      TRUE ~ 0
    )
  ) %>%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

total_ev_without_OT_tbl <- ev_without_OT_tbl %>%
  summarise(
    total_expected_attrition_cost_1 = sum(expected_attrition_cost)
  )
```

We observed that total expected cost with Overtime policy is $2,795,000. This implies cost is going down ( $3,191,435 to $2,795,000) after implementing the policy.

```{r, message=FALSE, warning=FALSE}
total_ev_without_OT_tbl
```

*Expected Savings Calculation*

* We can potentially save **$396k** for the organization  i.e 12.4% savings.
* To annualize the savings, multiply the factor of total observations to test observations.   
(train + test) / test = (1250 + 220) / 220 = 6.7.   
* Annualizing Savings: $396k X 6.7 = $2.6M.   

```{r, message=FALSE, warning=FALSE}

bind_cols(
  total_ev_with_OT_tbl,
  total_ev_without_OT_tbl
) %>%
  mutate(
    savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
    pct_savings = savings / total_expected_attrition_cost_0
  )
```

So, we can potentially save the organization $2.6M per year if we implement No overtime policy for everyone.  
**Targeting by Threshold Primer**  

* In this case, we **target by Threshold**, if threshold = 30%, only employees with prob_leave >= 0.30 are targeted.  
*  Most Critical : Don't want to miss FNs. Typically cost is higher for FNs.  

**Comparing FNs vs FPs**

* FN's are more costly than FPs  
*   **False Negatives** - We predict employee stays but actually leaves, we fail to target and do not reduce OT for 100% of attrition cost.  
*   **False Positives** - We predict leaves but stays, we target and reduce OT for 10-30% of attrition cost.  
* For example: Lets say FN = $100 k , FP = $30K ,FN/FP = 3X more costly. In this case, when FN's = 3X FP's, lower threshold (lower the threshold from 28% to 13% ,catch the people that have 13% or more probability of quitting.    
* Expected Savings Increases 13.9% ((549 - 482) / 482 * 100), $67k Savings / 0.15 = $446k per year additional savings for full population.    


```{r, message=FALSE, warning=FALSE}

performance_h2o <- automl_leader %>%
  h2o.performance(newdata = as.h2o(test_tbl))

performance_h2o %>%
  h2o.confusionMatrix()
```

Most Critical : Don't want to miss False Negatives. Typically cost is higher for FNs.

```{r, message=FALSE, warning=FALSE}

rates_by_threshold_tbl <- performance_h2o %>%
  h2o.metric() %>%
  as.tibble()

rates_by_threshold_tbl %>%
  glimpse()
```

F1 is the optimal balance between Precision & Recall.However, the threshold @Max F1 is not typically the optimal value for the business case because FNs are typically more costly than FPs.  

**tnr & fpr properties**  

1) Probability of correctly classifying a negative  
2) When actual is negative, we just sometimes classify it as positive  
3) tnr +  fpr = 1  


**tpr & fnr properties**

1) Probability of correctly classifying a positive  
2) When actual is positive, we just sometimes classify it as negative  
3) tpr +  fnr = 1  

```{r, message=FALSE, warning=FALSE}

rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  filter( f1 == max(f1)) %>%
  slice(1)  
```


**Visualizing Expected Rates**  

* Fundamentally, expected rates are the probability of getting the model prediction correct or incorrect at a given threshold.  
* Idea is to understand how expected rates interact as threshold increases.  
* **Cost Tradeoff** : In a perfect world, our model would never make mistakes & we could target TPs perfectly. We do not live in a perfect world, FPs and FNs results in costs.  

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 7}

rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  gather(key = key, value = value, tnr:tpr, factor_key = TRUE) %>%
  mutate(key = fct_reorder2(key, threshold, value)) %>%
  ggplot(aes(threshold, value, color = key)) +
  geom_point() + 
  geom_smooth() +
  theme_tq() +
  scale_color_tq() +
  theme(legend.position = "right") +
  labs(
    title = "Expected Rates",
    y = "Value",
    x = "Threshold"
  )

```

* We can notice from the plot that at any point on given threshold tnr and fpr are related, these rates together add up to 1  
* We can notice from the plot that at any point on given threshold tpr and fnr are related , these rates together add up to 1  

```{r, message=FALSE, warning=FALSE}

predictions_with_OT_tbl <- automl_leader %>%
  h2o.predict(newdata = as.h2o(test_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(EmployeeNumber, MonthlyIncome, OverTime)
  )

predictions_with_OT_tbl 


ev_with_OT_tbl <- predictions_with_OT_tbl %>%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %>%
  mutate(
    cost_of_policy_change = 0
  ) %>%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

ev_with_OT_tbl

total_ev_with_OT_tbl <- ev_with_OT_tbl %>%
  summarise(
    total_expected_attrition_cost_0 = sum(expected_attrition_cost)
  )

total_ev_with_OT_tbl
```


**Calculating Expected Value With Targeted OT**


```{r, message=FALSE, warning=FALSE}

max_f1_tbl <- rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  filter(f1 == max(f1)) %>%
  slice(1)

max_f1_tbl

tnr <- max_f1_tbl$tnr
fnr <- max_f1_tbl$fnr
fpr <- max_f1_tbl$fpr
tpr <- max_f1_tbl$tpr

threshold <- max_f1_tbl$threshold
threshold 
```

Anyone with Yes > =  0.3125333 and OverTime = Yes had their OT toggled to No.   
```{r, message=FALSE, warning=FALSE}

test_targeted_OT_tbl <- test_tbl %>%
  add_column(Yes = predictions_with_OT_tbl$Yes) %>%
  mutate(
    OverTime = case_when(
      Yes >= threshold ~ factor("No", levels = levels(test_tbl$OverTime)) ,
      TRUE ~ OverTime
     )
  ) %>%
  select(-Yes)

test_targeted_OT_tbl
```

Running h2o.predict() on the modified data will give us the probabilities of attrition = Yes i.e implementing the policy change.  
```{r, message=FALSE, warning=FALSE}

predictions_targeted_OT_tbl <- automl_leader %>%
  h2o.predict(newdata = as.h2o(test_targeted_OT_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(EmployeeNumber, MonthlyIncome, OverTime),
    test_targeted_OT_tbl %>%
      select(OverTime)
  ) %>%
  rename(
    OverTime_0 = OverTime,
    OverTime_1 = OverTime1
  )

predictions_targeted_OT_tbl
```


```{r, message=FALSE, warning=FALSE}

avg_overtime_pct <- 0.10


ev_targeted_OT_tbl <- predictions_targeted_OT_tbl %>%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %>%
  mutate(
    cost_of_policy_change = case_when(
      OverTime_0 == "Yes" & OverTime_1 == "No" ~ attrition_cost * avg_overtime_pct,
      TRUE ~ 0
    )
  ) %>%
  mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
  )


ev_targeted_OT_tbl

total_ev_targeted_OT_tbl <- ev_targeted_OT_tbl %>%
  summarise(total_expected_attrition_cost_1 = sum(expected_attrition_cost))

total_ev_targeted_OT_tbl

```

**Savings Calculation**

Percentage savings goes up to 14.2% from 12.4% , So targted OT makes more sense

```{r, message=FALSE, warning=FALSE}


savings_tbl <- bind_cols(
  total_ev_with_OT_tbl,
  total_ev_targeted_OT_tbl
) %>%
  mutate(
    savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
    pct_savings = savings / total_expected_attrition_cost_0
  )

savings_tbl
```

**Threshold Optimization**

* An Iterative approach to expected value.    
* Critical for maximizing profitability of a policy change.   

**calculate_savings_by_threshold()**

* A function that calculates & returns the savings when the user provides data, h2o model, threshold and expected rates (tnr, fpr, fnr, tpr)  
* Default: The function is set up for a *"No OT Policy"* because the threshold is so low
anyone with OT gets converted to No OT

**Savings By Threshold**

Function that can be used on a single threshold to calculate savings  
```{r, message=FALSE, warning=FALSE}

calculate_savings_by_threshold <- function(data, h2o_model, threshold = 0, 
                                           tnr = 0, fpr = 1, fnr = 0, tpr = 1) {
  
  data_0_tbl <- as.tibble(data)
  
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime)
    )
  
  ev_0_tbl <- pred_0_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = 250000
      )
    ) %>%
    mutate(
      cost_of_policy_change = 0
    ) %>%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl <- ev_0_tbl %>%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT
  data_1_tbl <- data_0_tbl %>%
    add_column(Yes = pred_0_tbl$Yes) %>%
    mutate(
      OverTime = case_when(
        Yes >= threshold ~ factor("No", levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %>%
    select(-Yes)
  
  pred_1_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime),
      data_1_tbl %>%
        select(OverTime)
    ) %>%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1
    )
  
  avg_overtime_pct <- 0.10
  
  ev_1_tbl <- pred_1_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = 250000
      )
    ) %>%
    mutate(
      cost_of_policy_change = case_when(
        OverTime_0 == "Yes" & OverTime_1 == "No" ~ attrition_cost * avg_overtime_pct,
        TRUE ~ 0
      )
    ) %>%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl <- ev_1_tbl %>%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  # Savings Calculation
  savings_tbl <- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %>%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}
```

**Max F1 Score : Threshold @ Max F1 i.e Target employees by balancing FN's and FP's**

Adjusting threshold to max F1 i.e targeting high risk employees which gives us savings which is bit more than what we had with No OT policy

```{r, message=FALSE, warning=FALSE}


# Threshold @ Max F1

rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  filter(f1 == max(f1))

```

Threshold @ Max F1 = 0.313

```{r, message=FALSE, warning=FALSE}

max_f1_savings <- calculate_savings_by_threshold(test_tbl, automl_leader, 
                                                 threshold = max_f1_tbl$threshold,
                                                 tnr = max_f1_tbl$tnr,
                                                 fnr = max_f1_tbl$fnr,
                                                 fpr = max_f1_tbl$fpr,
                                                 tpr = max_f1_tbl$tpr)
max_f1_savings
```

Savings in this case is $453K with threshold @ Max F1, more than No OT policy.  

**No OT Policy, Threshold = 0**

* Anyone With OT Targeted, analysis of test set (15% of Total Population).
* $396k / 0.15 = $2.6M per year savings.


```{r, message=FALSE, warning=FALSE}

# No OT Policy

test_tbl %>%
  calculate_savings_by_threshold(automl_leader, threshold = 0,
                                 tnr = 0, fnr = 0, tpr = 1, fpr = 1)
```


**Do Nothing Policy i.e threshold = 1**
 
```{r, message=FALSE, warning=FALSE}

 # Do Nothing Policy

test_tbl %>%
  calculate_savings_by_threshold(automl_leader, threshold = 1,
                                 tnr = 1, fnr = 1, tpr = 0, fpr = 0)
```

* No Savings in this case because threshold is so high that no one is targeted (No one has 100% probability of leaving)


**Optimize by Threshold** : **Maximizing Expected ROI**  

**Threshold @ Max Savings** : Target employees by weighted analysis of cost of FN and cost of FP  

* Generally, threshold @ F1 Max is default threshold for most classification algorithms 
* But it is not optimized for business case. F1 treats False positives & False negatives equally. They are not FN's are in many cases 3X+ more costly to organization  
*   **False Negatives** - We predict employee stays but actually leaves, we fail to target and do not reduce OT for 100% of attrition cost  
*   **False Positives** - We predict leaves but stays, we target and reduce OT for 10-30% of attrition cost  
* FN's are more costly than FPs Comparing FNs vs FPs
* For example: Lets say FN = $100 k , FP = $30K ,FN/FP = 3X more costly. In this case, when FN's = 3X FP's, lower threshold (lower the threshold from 28% to 13% ,catch the people that have 13% or more probability of quitting  

**Sampling Indices in test data**  

> When we perform an iterative process, this often takes a lot of time. We can sample the indices to reduce the number of iterations from 220 (no .of obs in test data) to 20 (sampled data) which reduces our iteration time by a factor of 11x.

**Iteratively Applying the function rates_by_threshold_optimized_tbl to optimize**  

```{r, message=FALSE, warning=FALSE}
smpl <-  seq(1, 220, length.out = 20) %>% round(digits = 0)

rates_by_threshold_optimized_tbl <- rates_by_threshold_tbl %>%
  select(threshold, tnr:tpr) %>%
  slice(smpl) %>%
  mutate(
    savings = pmap_dbl(.l = list(
      threshold = threshold,
      tnr = tnr,
      fnr = fnr,
      fpr = fpr,
      tpr = tpr
    ),
    .f = partial(calculate_savings_by_threshold, data = test_tbl, h2o_model = automl_leader)
    )
  )

rates_by_threshold_optimized_tbl
```


**Visualizing Optimized Savings**

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 12}

rates_by_threshold_optimized_tbl %>%
  ggplot(aes(threshold, savings)) +
  #Vlines
  geom_vline(xintercept = max_f1_tbl$threshold, 
             color = palette_light()[[5]] , size = 2) +
  geom_vline(aes(xintercept = threshold), 
             color = palette_light()[[3]] , size = 2,
            data =  rates_by_threshold_optimized_tbl %>%
              filter(savings ==  max(savings))
  ) +
  # Points
  geom_line(color = palette_light()[[1]]) +
  geom_point(color = palette_light()[[1]]) +
  
  # Optimal Point 
  geom_point(shape = 21, size = 5, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(savings == max(savings))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(savings == max(savings))) +
  # F1 Max
  geom_vline(xintercept = max_f1_tbl$threshold,
             color = palette_light()[[5]], size = 2) +
  annotate(geom = "label", label = scales::dollar(max_f1_savings),
           x = max_f1_tbl$threshold, y = max_f1_savings, vjust = -1,
           color = palette_light()[[1]]) +
  
  # No OT Policy
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(threshold == min(threshold))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(threshold == min(threshold))) +
  
  # Do Nothing
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(threshold == max(threshold))) +
  geom_label(aes(label = scales::dollar(round(savings,0))), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(threshold == max(threshold))) +
  
  # Aesthestics 
  theme_tq() +
  expand_limits(x = c(-.1, 1.1), y = 8e5) + 
  scale_x_continuous(labels  = scales::percent,
                     breaks = seq(0, 1, by = 0.2)) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Optimization Results : Expected Savings Maximized At 13.3% ",
       x = "Threshold (%)", 
       y = "Savings")
```

* Lower the threshold from 31% to 14.2%: Catch the people that have 14.2% or more probability of quitting.
* Expected Savings Increases 17.4% ((532 - 453) / 453 * 100).
* $79k Savings / 0.15 = $526k per year additional savings for full population.
* Targeting employees with Yes > = 14.2% will maximize expected savings i.e **$532,600**.
* Big Savings : Targeting employees with Yes > = 14.2% , Increased savings by 17.4% versus F1.

**Sensitivity Analysis**

**Modelling Assumptions**: 

1. You will never have perfect information and therefore assumptions must be made.    
2. Always test you assumptions: Even though you need to test some assumptions, you can still try different values to test your assumptions. This is why we do **Sensitivity Analysis**.  

```{r, message=FALSE, warning=FALSE}

calculate_savings_by_threshold_2 <- function(data, h2o_model, threshold = 0, 
                                             tnr = 0, fpr = 1, fnr = 0, tpr = 1,
                                             avg_overtime_pct = 0.10,
                                             net_revenue_per_employee = 250000) {
  
  data_0_tbl <- as.tibble(data)
  
 
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime)
    )
  
  ev_0_tbl <- pred_0_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        # changed in _2 -----
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %>%
    mutate(
      cost_of_policy_change = 0
    ) %>%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl <- ev_0_tbl %>%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT
  
  data_1_tbl <- data_0_tbl %>%
    add_column(Yes = pred_0_tbl$Yes) %>%
    mutate(
      OverTime = case_when(
        Yes >= threshold ~ factor("No", levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %>%
    select(-Yes) 
  
  pred_1_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime),
      data_1_tbl %>%
        select(OverTime)
    ) %>%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1
    )
  

  avg_overtime_pct <- avg_overtime_pct 
  
  
  ev_1_tbl <- pred_1_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %>%
    mutate(
      cost_of_policy_change = case_when(
        OverTime_0 == "Yes" & OverTime_1 == "No" ~ attrition_cost * avg_overtime_pct,
        TRUE ~ 0
      )
    ) %>%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl <- ev_1_tbl %>%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  #Savings Calculation
  savings_tbl <- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %>%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}
```

```{r, message=FALSE, warning=FALSE}

test_tbl %>%
  calculate_savings_by_threshold_2(automl_leader, avg_overtime_pct = 0.10,
                                   net_revenue_per_employee = 250000)
```

Savings are reduced

```{r, message=FALSE, warning=FALSE}

test_tbl %>%
  calculate_savings_by_threshold_2(automl_leader, avg_overtime_pct = 0.15,
                                   net_revenue_per_employee = 300000)
```

**Sensitivity Analysis for OverTime**


**Classifier Calibration**: This combination of threshold and expected rates settings has our classifier
calibrated to optimum FN / FP Ratio for max savings. If cost / benefit change, settings need to be recalibrated (re-optimized)

```{r, message=FALSE, warning=FALSE}

max_savings_rates_tbl <- rates_by_threshold_optimized_tbl %>%
  filter(savings == max(savings))

max_savings_rates_tbl

calculate_savings_by_threshold_2(
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl$tnr,
  fnr =  max_savings_rates_tbl$fnr,
  fpr = max_savings_rates_tbl$fpr,
  tpr =  max_savings_rates_tbl$tpr
)

# Preloaded Function : Preloads the calibrated settings that optimize threshold &  maximize expected savings 

calculate_savings_by_threshold_2_preloaded <- partial(
  calculate_savings_by_threshold_2,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl$tnr,
  fnr =  max_savings_rates_tbl$fnr,
  fpr = max_savings_rates_tbl$fpr,
  tpr =  max_savings_rates_tbl$tpr
  
)


calculate_savings_by_threshold_2_preloaded(
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000)
```

Multiple Combinations: We have two inputs that we are simultaneously going to change:  

1. **Average OverTime Percent** : If an employee works 100% they essentially double their hours. We 
don't expect this to be average. Rather, it's likely that the worst case is 30% 
or roughly 30% of 40 = 12 hours per week

2. **Net Revenue per Employee(NRPE)** : On an income statement, take the gross revenue minus Costs of
Goods sold to get Net Revenue. They spread this out across every employee to get an 
estimate of their financial value to organization.  

**Worst Case**: We believe $200,000  for lowest NRPE  
**Best Case**:  We believe $400,000 for highest NRPE  

```{r, message=FALSE, warning=FALSE}

sensitivity_tbl <- list(
  avg_overtime_pct = seq(0.05, 0.30, by = 0.05), 
  net_revenue_per_employee = seq(200000, 400000, by = 50000)
) %>%
  cross_df() %>%
  mutate(
    savings = pmap_dbl(
      .l = list(
        avg_overtime_pct = avg_overtime_pct,
        net_revenue_per_employee = net_revenue_per_employee
      ),
      .f = calculate_savings_by_threshold_2_preloaded
    )
  )

sensitivity_tbl

```

**Heat Maps** are a great way to show how two variables interact with a third variable (the target variable)

```{r, message=FALSE, warning=FALSE}

sensitivity_tbl %>%
  ggplot(aes(avg_overtime_pct, net_revenue_per_employee)) +
  geom_tile(aes(fill = savings)) +
  geom_label(aes(label = savings %>% round(0) %>% scales::dollar())) +
  theme_tq() +
  theme(legend.position = "none") +
  scale_fill_gradient2(
    low = palette_light()[[2]],
    mid = "white",
    high = palette_light()[[1]],
    midpoint = 0
  ) + 
  scale_x_continuous(
    labels = scales::percent,
    breaks = seq(0.05, 0.30, by = 0.05)
  ) +
  scale_y_continuous(
    labels = scales::dollar
  ) +
  labs(
    title = "Profitability Heatmap : Expected Savings Sensitivity Analysis ",
    subtitle = "How sensitive is savings to net revenue per employee and average overtime percentage ?",
    x = "Average Overtime Percentage",
    y = "Net Revenue Per Employee"
  )
```

As long as people are not working OverTime more than 25%, we are break even that is profit.  

**Threshold Optimization For Stock Options**

**Challenge : People with no stock options are leaving**

**Find optimal threshold**

```{r, message=FALSE, warning=FALSE}


avg_overtime_pct <- 0.10
net_revenue_per_employee <- 250000
stock_option_cost <- 5000

data <- test_tbl 
h2o_model <- automl_leader

calculate_savings_by_threshold_3 <- function(data, h2o_model, threshold = 0, 
                                             tnr = 0, fpr = 1, fnr = 0, tpr = 1,
                                             avg_overtime_pct = 0.10,
                                             net_revenue_per_employee = 250000,
                                             stock_option_cost = 5000) {
  
  data_0_tbl <- as.tibble(data)
  
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime, StockOptionLevel)
    )
  
  ev_0_tbl <- pred_0_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %>%
    mutate(
      cost_of_policy_change = 0
    ) %>%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl <- ev_0_tbl %>%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT & Stock Option Policy
  
  data_1_tbl <- data_0_tbl %>%
    add_column(Yes = pred_0_tbl$Yes) %>%
    mutate(
      OverTime = case_when(
        Yes >= threshold ~ factor("No", levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %>%
    mutate(
      StockOptionLevel = case_when(
        Yes >= threshold & StockOptionLevel == 0 
        ~ factor("1", levels = levels(data_0_tbl$StockOptionLevel)) ,
        TRUE ~ StockOptionLevel
      )
    ) %>%
    select(-Yes) 
  
  pred_1_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime, StockOptionLevel),
      data_1_tbl %>%
        select(OverTime, StockOptionLevel)
    ) %>%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1,
      StockOptionLevel_0 = StockOptionLevel,
      StockOptionLevel_1 = StockOptionLevel1   
    )
  

  avg_overtime_pct <- avg_overtime_pct 
  stock_option_cost <- stock_option_cost 
  
  
  ev_1_tbl <- pred_1_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %>%
    # cost OT
    mutate(
      cost_OT = case_when(
        OverTime_0 == "Yes" & OverTime_1 == "No"
        ~  avg_overtime_pct * MonthlyIncome * 12,
        TRUE ~ 0
      )
    ) %>%
    # cost Stock Option
    mutate(
      cost_SO = case_when(
        StockOptionLevel_1 == "1" & StockOptionLevel_0 == "0"
        ~ stock_option_cost,
        TRUE ~ 0
      )
    ) %>%
    mutate(cost_of_policy_change = cost_OT + cost_SO) %>%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl <- ev_1_tbl %>%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  # Savings Calculation
  savings_tbl <- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %>%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}

```

```{r, message=FALSE, warning=FALSE}


max_f1_tbl <- rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  filter(f1 == max(f1))

max_f1_savings <- calculate_savings_by_threshold_3(
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_f1_tbl$threshold,
  tnr = max_f1_tbl$tnr,
  fnr =  max_f1_tbl$fnr,
  fpr = max_f1_tbl$fpr,
  tpr =  max_f1_tbl$tpr,
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000
)
```

**Optimisation**

```{r, message=FALSE, warning=FALSE}

smpl <- seq(1, 220, length.out = 20) %>% round(digits = 0)
  
calculate_savings_by_threshold_3_preloaded <- partial(
  calculate_savings_by_threshold_3,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000)

rates_by_threshold_optimized_tbl_3 <- rates_by_threshold_tbl %>%
  select(threshold, tnr:tpr) %>%
  slice(smpl) %>%
  mutate(
    savings = pmap_dbl(.l = list(
      threshold = threshold,
      tnr = tnr,
      fnr = fnr,
      fpr = fpr,
      tpr = tpr
    ),
    .f = calculate_savings_by_threshold_3_preloaded
    )
  )

rates_by_threshold_optimized_tbl_3
```

```{r, message=FALSE, warning=FALSE}

rates_by_threshold_optimized_tbl_3 %>%
  filter(savings == max(savings))
 
```


```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}

rates_by_threshold_optimized_tbl_3 %>%
  ggplot(aes(threshold, savings)) +
  
  #Vlines
  geom_vline(xintercept = max_f1_tbl$threshold, 
             color = palette_light()[[5]] , size = 2) +
  geom_vline(aes(xintercept = threshold), 
             color = palette_light()[[3]] , size = 2,
            data =  rates_by_threshold_optimized_tbl_3 %>%
              filter(savings ==  max(savings))
  ) +
  # Points
  geom_line(color = palette_light()[[1]]) +
  geom_point(color = palette_light()[[1]]) +
  
  # F1 Max
  annotate(geom = "label", label = scales::dollar(max_f1_savings),
           x = max_f1_tbl$threshold, y = max_f1_savings, vjust = -1,
           color = palette_light()[[1]]) +
  
  # Optimal Point 
  geom_point(shape = 21, size = 5, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(savings == max(savings))) +
  
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -2, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(savings == max(savings))) +

  # No OT Policy
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(threshold == min(threshold))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(threshold == min(threshold))) +
  
  # Do Nothing
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(threshold == max(threshold))) +
  geom_label(aes(label = scales::dollar(round(savings,0))), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(threshold == max(threshold))) +
  
  # Aesthestics 
  theme_tq() +
  expand_limits(x = c(-.1, 1.1), y = 12e5) + 
  scale_x_continuous(labels  = scales::percent,
                     breaks = seq(0, 1, by = 0.2)) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Optimization Results : Expected Savings Maximized At 26.7% ",
       x = "Threshold (%)", 
       y = "Savings")

```


**Sensitivity Analysis at Optimal Threshold**


```{r, message=FALSE, warning=FALSE}

net_revenue_per_employee <- 250000
avg_overtime_pct <- seq(0.05, 0.30, by = 0.05)
stock_option_cost <- seq(5000, 25000, by = 5000)

max_savings_rates_tbl_3 <- rates_by_threshold_optimized_tbl_3 %>%
filter(savings == max(savings))

max_savings_rates_tbl_3
```


```{r, message=FALSE, warning=FALSE}
calculate_savings_by_threshold_3_preloaded <- partial(
  calculate_savings_by_threshold_3,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl_3$tnr,
  fnr =  max_savings_rates_tbl_3$fnr,
  fpr = max_savings_rates_tbl_3$fpr,
  tpr =  max_savings_rates_tbl_3$tpr
  
)
```

```{r, message=FALSE, warning=FALSE}

calculate_savings_by_threshold_3_preloaded(
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000)  
```


```{r, message=FALSE, warning=FALSE}

sensitivity_tbl_3 <- list(
  avg_overtime_pct  = seq(0.05, 0.30, by = 0.05),
  net_revenue_per_employee = 250000,
  stock_option_cost = seq(5000, 25000, by = 5000)
) %>%
  cross_df() %>%
  mutate(
    savings = pmap_dbl(
      .l = list(
        avg_overtime_pct = avg_overtime_pct,
        net_revenue_per_employee = net_revenue_per_employee,
        stock_option_cost = stock_option_cost
      ),
      .f = calculate_savings_by_threshold_3_preloaded
    )
  )

sensitivity_tbl_3
```

```{r, message=FALSE, warning=FALSE}
sensitivity_tbl_3 %>%
  ggplot(aes(avg_overtime_pct, stock_option_cost)) +
  geom_tile(aes(fill = savings)) +
  geom_label(aes(label = savings %>% round(0) %>% scales::dollar())) +
  theme_tq() +
  theme(legend.position = "none") +
  scale_fill_gradient2(
    low = palette_light()[[2]],
    mid = "white",
    high = palette_light()[[1]],
    midpoint = 0
  ) + 
  scale_x_continuous(
    labels = scales::percent,
    breaks = seq(0.05, 0.30, by = 0.05)
  ) +
  scale_y_continuous(
    labels = scales::dollar,
    breaks = seq(5000, 25000, by = 5000)
  ) +
  labs(
    title = "Profitability Heatmap : Expected Savings Sensitivity Analysis ",
    subtitle = "How sensitive is savings to stock options cost and average overtime percentage ?",
    x = "Average Overtime Percentage",
    y = "Average Stock Options Cost"
  )
```

As longs as average overtime percentage is less than 18% and avg stocks options cost less than
$20,000, we are break even that is profit.

**Recommendation Algorithm Strategy**

**Purpose** 
To enable immediate manager to reduce attrition using an automated recommendation system  

**Strategy Logic:**

* Uses Correlation Analysis to determine logical strategies that can be adjusted via stakeholder feedback  
* Investigate your strategies and try to put them into 2 or 3 groups. This helps managers focus on specific areas.  


 ! [Alt text] (/Untitled/Users/raj/Pictures/Snip20190109_2.png)


**Strategy Groups**

* **Work Life** : OverTime, BusinessTravel
* **Personal Development** : Traning & Mentoring Employees, Sponsoring Higher Education
* **Professional Development**:   Promotion based on experience & Job level


**Recipes for Correlation Analysis**

After applying recipe, we get a discretized Output i.e all are binary features 0's and 1's only. This discretization of features will help us in identifying strategies. 

```{r, message=FALSE, warning=FALSE}

recipe_obj <- recipe(Attrition ~ . , data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_num2factor(factor_names) %>%
  step_discretize(all_numeric(), options = list(min_unique = 1)) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  prep()
```

```{r, message=FALSE, warning=FALSE}
train_corr_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
train_corr_tbl %>% glimpse()
```

**Data Manipulation for Correlation Visualization**

```{r, message=FALSE, warning=FALSE}

cor_level <- 0.06

correlation_results_tbl <- train_corr_tbl %>%
  select(-Attrition_No) %>%
  get_cor(Attrition_Yes, fct_reorder = TRUE, fct_rev = TRUE) %>%
  filter(abs(Attrition_Yes) >= cor_level) %>%
  mutate(
    relationship = case_when(
      Attrition_Yes > 0 ~ "Supports",
      TRUE ~ "Contradicts"
    )
  ) %>%
  mutate(feature_text = as.character(feature)) %>%
  separate(feature_text, into = "feature_base", sep = "_", extra = "drop") %>%
  mutate(feature_base = as_factor(feature_base) %>% fct_rev())


length_unique_groups <- correlation_results_tbl %>%
  pull(feature_base) %>% 
  unique() %>%
  length()
```

```{r, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 14}  

correlation_results_tbl %>%
  ggplot(aes(Attrition_Yes, feature_base, color = relationship)) +
  geom_point() + 
  geom_label(aes(label = feature), vjust = -0.5) +
  expand_limits(x = c(-0.3, 0.3), y = c(1,length_unique_groups + 1)) +
  theme_tq() +
  scale_color_tq() +
  labs(
    title = "Correlation Analysis : Recommendation Strategy Development",
    subtitle =  "Discretizing feature to help identify a strategy"
    )
```

**Recommendation Strategy Development**

* Implementing Strategies into code  
* We will be using **Good Better Best Approach** for building a recommendation strategy development   

Function **recommend_strategies**  evaluates recommendation startegies given an employee number  
```{r, message=FALSE, warning=FALSE}

recommend_strategies <- function(data, employee_number){
  
  data %>%
    filter(EmployeeNumber == employee_number) %>%
    mutate_if(is.factor, as.numeric) %>%
    
    # Personal Development Strategy 
    mutate(
      personal_development_strategy =  case_when(
        
        # (Worst Case) Create Personal Development Plan: Job Involvement, JobSatisfaction, PerformanceRating 
        PerformanceRating == 1 | 
          JobSatisfaction == 1 |
          JobInvolvement  <= 2      ~  "Create Personal Development Plan",
        # (Better Case) Promote Training & Formation: YearsAtCompany, TotalWorkingYears	
        YearsAtCompany < 3 |
          TotalWorkingYears < 6    ~  "Promote Training & Formation" ,
        
        # (Best Case 1) Seek Mentorship Role: YearsInCurrentRole, YearsAtCompany, PerformanceRating, JobSatisfaction
        (YearsInCurrentRole > 3 |  YearsAtCompany >= 5) & 
          PerformanceRating >= 3 & 
          JobSatisfaction == 4  ~  "Seek Mentorship Role",
        
        
        # (Best Case 2) Seek Leadership Role : JobInvolvement, JobSatisfaction, PerformanceRating
        JobInvolvement >= 3 & 
          JobSatisfaction >= 3 &
          PerformanceRating>= 3  ~ "Seek Leadership Role",
        
        # Catch All
        TRUE ~ "Retain and Maintain"
      )
    ) %>%
    # select(EmployeeNumber, personal_development_strategy)
  
  
    # Professional Development Strategy
  mutate(
    professional_development_strategy =  case_when(
      
      #  Ready for Rotation: YearsInCurrentRole,  JobSatisfaction (LOW)
      YearsInCurrentRole >= 2 &
        JobSatisfaction <= 2                 ~"Ready for Rotation",
      
      #  Ready for Promotion Level 2: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 1 &
        YearsInCurrentRole >=2 &
        JobInvolvement >=3 &
        PerformanceRating >=3                 ~ "Ready for Promotion",   
      
      #  Ready for Promotion Level 3: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 2 &
        YearsInCurrentRole >=2 &
        JobInvolvement >=4 &
        PerformanceRating >=3                 ~ "Ready for Promotion",   
      
      #  Ready for Promotion Level 4: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 3 &
        YearsInCurrentRole >=3 &
        JobInvolvement >=4 &
        PerformanceRating >=3                 ~ "Ready for Promotion",   
      
      #  Ready for Promotion Level 5: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 4 &
        YearsInCurrentRole >=4 &
        JobInvolvement >=4 &
        PerformanceRating >=3                 ~ "Ready for Promotion",   
      
      #  Incentivize Specialization : YearsInCurrentRole, JobSatisfaction, PerformanceRating
      YearsInCurrentRole >= 4 & 
        JobSatisfaction >= 4 &
        PerformanceRating >= 3         ~ "Incentivize Specialization",
      
      # Catch All
      TRUE ~ "Retain and Maintain"
    )
  ) %>%
  #select(EmployeeNumber, personal_development_strategy,professional_development_strategy)  
    
   # Work-Environment Strategy
    mutate(
      work_environment_strategy =  case_when(
        
        # Improve Work - Life Balance : OverTime, WorkLifeBalance
        OverTime == 2 | 
          WorkLifeBalance == 1                 ~ "Improve Work-Life Balance",
        
        # Monitor Business Travel: BusinessTravel, DistanceFromHome, WorkLifeBalance	
        ( BusinessTravel == 3|
            DistanceFromHome >= 10) &
          WorkLifeBalance == 2                 ~ "Monitor Business Travel",
        
        
        # Review Job Assignment: EnvironmentSatisfaction, YearsInCurrentRole 
        EnvironmentSatisfaction == 1 &
          YearsInCurrentRole >= 2             ~ "Review Job Assignment",
        
        # Promote Job Engagement: JobInvolvement
        JobInvolvement <= 2   ~ "Promote Job Engagement",
        
        # Catch All
        TRUE ~ "Retain and Maintain"
      )
    ) %>%
    select(EmployeeNumber, personal_development_strategy,professional_development_strategy, work_environment_strategy)
}
```


Now, we will evaluate strategies for employees both in training and test data.

Recommendation strategies for *Employee number 4* from training data:  

```{r, message=FALSE, warning=FALSE}

train_readable_tbl %>%
  recommend_strategies(4)
```

Recommendation strategies for *Employee number 228* from test data : 

```{r, message=FALSE, warning=FALSE}

test_readable_tbl %>%
  recommend_strategies(228)
```

## Phase 6 : Deployment

In this phase, we can deploy a nice formatted dashboard or shiny apps for stakeholders to take decisions given an Employee number
(To Be Continued...)

# Thoughts 

# Inspired From  

* Matt Dancho - [Business Science University](https://www.business-science.io/)

# Disclaimer  

I am not authorized, endorsed by, or in any way officially connected with H2O.ai. The views are mine not my employers. 